{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anassdrs/plant_identification/blob/main/plants.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd  # data processing\n",
        "import os #  to interact with files using there paths\n",
        "import cv2\n",
        "from sklearn.datasets import load_files\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "2aGEUcNn8fMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7cO7q6AKwBtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Unzip the uploaded data into google drive \n",
        "#!unzip \"/content/drive/MyDrive/les_photos_de_validation_ayoub\" -d \"/content/drive/MyDrive\""
      ],
      "metadata": {
        "id": "zpf0yjGKt6Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0BuCfBXR8Xdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to a directory with image dataset and subfolders for training, validation and final testing\n",
        "DATA_PATH = r\"drive/MyDrive\"\n",
        "\n",
        "# Number of threads for data loader \n",
        "NUM_WORKERS = 4  \n",
        "\n",
        "# Image size: even though image sizes are bigger than 64, we use this to speed up training\n",
        "SIZE_H = SIZE_W = 350\n",
        "\n",
        "# Number of classes in the dataset\n",
        "NUM_CLASSES = len(sorted(os.listdir(os.path.join(DATA_PATH + '/train'))))\n",
        "print(NUM_CLASSES)\n",
        "# Epochs: number of passes over the training data, we use it this small to reduce training babysitting time\n",
        "EPOCH_NUM = 10\n",
        "\n",
        "# Batch size: for batch gradient descent optimization, usually selected as 2**K elements\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Images mean and std channelwise\n",
        "image_mean = [0, 0, 0]\n",
        "image_std  = [1, 1, 1]\n",
        "\n",
        "# # Last layer (embeddings) size for CNN models\n",
        "# EMBEDDING_SIZE = 128"
      ],
      "metadata": {
        "id": "z4VFngiByK0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "DATA_PATH=\"drive/MyDrive\"\n",
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "i = 1\n",
        "for stage in ['train', 'val', 'les_photos_de_validation']:\n",
        "    count_samples_for_class = dict()\n",
        "    for directory in tqdm(sorted(os.listdir(os.path.join(DATA_PATH + '/' + stage)))):\n",
        "        count_samples_for_class[directory] = len(os.listdir(os.path.join(DATA_PATH + '/' + stage + '/' + directory)))\n",
        "    plt.subplot(2,2,i)\n",
        "    plt.bar(count_samples_for_class.keys(), count_samples_for_class.values(), width=20, color='g', label=stage)\n",
        "    plt.legend()\n",
        "    i+=1\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R1MjKYt_wggY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = transforms.Compose([\n",
        "    transforms.Resize((350, 350)),       # scaling images to fixed size\n",
        "    transforms.ToTensor(),                      # converting to tensors\n",
        "    transforms.Normalize(image_mean, image_std) # normalize image data per-channel\n",
        "])"
      ],
      "metadata": {
        "id": "80Q74oaYxWNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset using torchvision.datasets.ImageFolder\n",
        "train_dataset = torchvision.datasets.ImageFolder(os.path.join(DATA_PATH, 'train'), transform=transformer)\n",
        "val_dataset = torchvision.datasets.ImageFolder(os.path.join(DATA_PATH, 'val'), transform=transformer)\n",
        "# load test data also, to be used for final evaluation\n",
        "test_dataset = torchvision.datasets.ImageFolder(os.path.join(DATA_PATH, 'les_photos_de_validation'), transform=transformer)"
      ],
      "metadata": {
        "id": "LLGGMiR5xQPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_train, n_val, n_test = len(train_dataset), len(val_dataset), len(test_dataset)\n",
        "n_train, n_val, n_test"
      ],
      "metadata": {
        "id": "3hxpmRyjx7f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                              batch_size=BATCH_SIZE,\n",
        "                                              shuffle=True,\n",
        "                                              num_workers=NUM_WORKERS)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                                            batch_size=BATCH_SIZE,\n",
        "                                            shuffle=True,\n",
        "                                            num_workers=NUM_WORKERS)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             num_workers=NUM_WORKERS)\n",
        "classes = train_dataset.classes\n",
        "print(classes)\n",
        "print(type(classes))\n",
        "import json\n",
        "\n",
        "\n",
        "with open('drive/MyDrive/classes.json', 'w') as jsonfile:\n",
        "    json.dump(classes, jsonfile)\n",
        "\n",
        "#encoder and decoder to convert classes into integer\n",
        "decoder = {}\n",
        "for i in range(len(classes)):\n",
        "    decoder[classes[i]] = i\n",
        "encoder = {}\n",
        "for i in range(len(classes)):\n",
        "    encoder[i] = classes[i]\n",
        "print(type(train_loader))\n",
        "x,y=next(iter(train_loader))\n",
        "print(len(y))"
      ],
      "metadata": {
        "id": "gt6aXF6Vx_1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_from_batch_generator(batch_gen):\n",
        "    data_batch, label_batch = next(iter(batch_gen))\n",
        "    grid_size = (3, 3)\n",
        "    f, axarr = plt.subplots(*grid_size)\n",
        "    f.set_size_inches(30,20)\n",
        "    class_names = batch_gen.dataset.classes\n",
        "    for i in range(grid_size[0] * grid_size[1]):\n",
        "        # read images from batch to numpy.ndarray and change axes order [H, W, C] -> [H, W, C]\n",
        "        batch_image_ndarray = np.transpose(data_batch[i].numpy(), [1, 2, 0])\n",
        "        # inverse normalization for image data values back to [0,1] and clipping the values for correct pyplot.imshow()\n",
        "        src = np.clip(image_std * batch_image_ndarray + image_mean, 0, 1)\n",
        "        \n",
        "        # display batch samples with labels\n",
        "        sample_title = '(%s)' % ( class_names[label_batch[i]])\n",
        "        axarr[i // grid_size[0], i % grid_size[0]].imshow(src)\n",
        "        axarr[i // grid_size[0], i % grid_size[0]].set_title(sample_title)\n",
        "    pass"
      ],
      "metadata": {
        "id": "FoaF0sQPyfH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_from_batch_generator(train_loader)\n"
      ],
      "metadata": {
        "id": "xtFNJodiyl3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install efficientnet_pytorch"
      ],
      "metadata": {
        "id": "-GgWyB1rywnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "import tensorflow as tf\n",
        "IMG_SHAPE = (350, 350, 3) \n",
        "model0 = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights=\"imagenet\")\n",
        "tf.keras.utils.plot_model(model0) # pour dessiner et visualiser \n",
        "a=model0.summary() # pour voir la liste des couches et des paramÃ¨tres\n",
        "plot_model(model0, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "vz5AwumG3Acg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "model = EfficientNet.from_pretrained('efficientnet-b4')\n",
        "\n",
        "# Disable gradient updates for all the layers except the final layer\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_feat = model._fc.in_features\n",
        "model._fc = nn.Linear(num_feat, NUM_CLASSES)\n",
        "\n",
        "# Use available device for calculations\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "U908HY31yz95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm.auto import tqdm\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "\n",
        "def fit(model, augmentation, shedul, epochs, lr):\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "    \n",
        "    if shedul==1:\n",
        "        scheduler = lr_scheduler.StepLR(opt, step_size=1, gamma=0.8)\n",
        "    \n",
        "    \n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracy = []\n",
        "    val_accuracy = []\n",
        "    start = time.time()\n",
        "    a_train=[]\n",
        "    a_val=[]\n",
        "    l_train=[]\n",
        "    l_val=[]\n",
        "    for epoch in range(epochs):\n",
        "        correct = []\n",
        "        loss1=[]\n",
        "        model.train(True)\n",
        "        for img, label in train_loader:\n",
        "            clear_output(True)\n",
        "            img = img.to(device)\n",
        "            label = label.to(device)\n",
        "            opt.zero_grad()\n",
        "            preds = model(img).to(device)\n",
        "            ce_loss = nn.CrossEntropyLoss()\n",
        "            loss = ce_loss(preds, label)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            tmp_loss = loss.item()\n",
        "            train_losses.append(tmp_loss)\n",
        "\n",
        "            end = time.time() - start\n",
        "            if shedul==1:\n",
        "                print(f'epoch = {epoch}, Learning rate =', scheduler.get_lr())\n",
        "\n",
        "            acc = accuracy_score(label.cpu(), np.argmax(preds.cpu().detach().numpy(), axis=1))\n",
        "            train_accuracy.append(acc)\n",
        "            correct.append(acc)\n",
        "            loss1.append(tmp_loss)\n",
        "            plt.figure(figsize=(15,13))\n",
        "            plt.subplot(2,1,1)\n",
        "            plt.plot(np.arange(0, len(train_losses)), train_losses, label=f'spent_time={end}, ep={epoch}')\n",
        "            plt.legend()\n",
        "\n",
        "            plt.subplot(2,1,2)\n",
        "            plt.plot(np.arange(0, len(train_accuracy)), train_accuracy, label='train_accuracy')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "        accuracy = sum(correct)/len(correct)\n",
        "        l= sum(loss1)/len(loss1)\n",
        "        # trainset, not train_loader\n",
        "        # probably x in your case\n",
        "        a_train.append(accuracy)\n",
        "        l_train.append(l)\n",
        "        print(\"Accuracy = {}\".format(a_train))\n",
        "        if augmentation == 1:\n",
        "            for img, label in train_loader_aug:\n",
        "                clear_output(True)\n",
        "                img = img.to(device)\n",
        "                label = label.to(device)\n",
        "                opt.zero_grad()\n",
        "                preds = model(img).to(device)\n",
        "                ce_loss = nn.CrossEntropyLoss()\n",
        "                loss = ce_loss(preds, label)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "                tmp_loss = loss.item()\n",
        "                train_losses.append(tmp_loss)\n",
        "\n",
        "                end = time.time() - start\n",
        "\n",
        "                acc = accuracy_score(label.cpu(), np.argmax(preds.cpu().detach().numpy(), axis=1))\n",
        "                train_accuracy.append(acc)\n",
        "              \n",
        "                plt.figure(figsize=(15,13))\n",
        "                plt.subplot(2,1,1)\n",
        "                plt.plot(np.arange(0, len(train_losses)), train_losses, label=f'aug| spent_time={end}, ep={epoch}')\n",
        "                plt.legend()\n",
        "\n",
        "                plt.subplot(2,1,2)\n",
        "                plt.plot(np.arange(0, len(train_accuracy)), train_accuracy, label='aug| train_accuracy')\n",
        "                plt.legend()\n",
        "                plt.show()\n",
        "                \n",
        "        if shedul==1:\n",
        "            scheduler.step()\n",
        "        model.train(False)\n",
        "        with torch.no_grad():\n",
        "            correct = []\n",
        "            loss1=[]\n",
        "            for img, label in tqdm(val_loader):\n",
        "                clear_output(True)\n",
        "                img = img.to(device)\n",
        "                label = label.to(device)\n",
        "                \n",
        "                preds = model(img).to(device)\n",
        "                ce_loss = nn.CrossEntropyLoss()\n",
        "                loss = ce_loss(preds, label)\n",
        "                tmp_loss = loss.item()\n",
        "                val_losses.append(tmp_loss)\n",
        "\n",
        "                acc = accuracy_score(label.cpu(), np.argmax(preds.cpu().detach().numpy(), axis=1))\n",
        "                val_accuracy.append(acc)\n",
        "                correct.append(acc)\n",
        "                loss1.append(tmp_loss)\n",
        "                plt.figure(figsize=(15,13))\n",
        "                plt.subplot(2,1,1)\n",
        "                plt.plot(np.arange(0, len(val_losses)), val_losses, label=f'ep={epoch}')\n",
        "                plt.legend()\n",
        "\n",
        "                plt.subplot(2,1,2)\n",
        "                plt.plot(np.arange(0, len(val_accuracy)), val_accuracy, label='val_accuracy')\n",
        "                plt.legend()\n",
        "                plt.show()\n",
        "            accuracy = sum(correct)/len(correct)\n",
        "            l=sum(loss1)/len(loss1)\n",
        "            # trainset, not train_loader\n",
        "            # probably x in your case\n",
        "            a_val.append(accuracy)\n",
        "            l_val.append(l)\n",
        "            print(\"Accuracy = {}\".format(a_val))\n",
        "    return model, train_losses, train_accuracy, val_losses, val_accuracy, a_train,a_val,l_train,l_val"
      ],
      "metadata": {
        "id": "IJois2YEzWy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qvkfrQJoaHDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCH_NUM=30\n",
        "augmentation=0\n",
        "shedul=0\n",
        "model1, train_losses, train_accuracy, val_losses, val_accuracy ,a_train,a_val,l_train,l_val= fit(model, augmentation, shedul, EPOCH_NUM , lr=0.003)"
      ],
      "metadata": {
        "id": "4LjuAf37zh2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(a_train)\n",
        "print(a_val)"
      ],
      "metadata": {
        "id": "iG-brWLQceSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_train=[0.3960703822307011, 0.5233997070245964, 0.5459945833617225, 0.5497334264495469, 0.5591955099816038, 0.5725114124139811, 0.577344654902228, 0.5774936976221299, 0.5751728895550862, 0.5833787558765415, 0.5837194249506029, 0.5823397152006541, 0.5916272058322545, 0.5851076514274034, 0.592040267084554, 0.589727975744362, 0.5940587313483683, 0.5929515568576685, 0.5914440962049465, 0.5921637596239013, 0.593603086461811, 0.595238298017306, 0.5944760509640935, 0.5925257205150917, 0.5937563875451387, 0.5938372964502283, 0.6007188117462696, 0.5929685903113715, 0.6000246985078694, 0.6030183279961845]\n",
        "accuracy_val=[0.5469858156028369, 0.5877659574468085, 0.5842198581560284, 0.5966312056737588, 0.5886524822695035, 0.5953014184397163, 0.5970744680851063, 0.600177304964539, 0.6006205673758865, 0.6032801418439716, 0.6063829787234043, 0.6085992907801419, 0.6112588652482269, 0.613031914893617, 0.612145390070922, 0.6139184397163121, 0.6236702127659575, 0.6099290780141844, 0.6117021276595744, 0.6054964539007093, 0.6134751773049646, 0.6117021276595744, 0.612145390070922, 0.6187943262411347, 0.6148049645390071, 0.6205673758865248, 0.6148049645390071, 0.6125886524822695, 0.6108156028368794, 0.6174645390070922]\n",
        "epochs = range(0,30)\n",
        "print(max(accuracy_val))\n",
        "plt.plot(epochs, accuracy_train, 'b', label='accuracy_train')\n",
        "plt.plot(epochs, accuracy_val, 'y', label='accuracy_val')\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W-0VoAVmrh7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_results(train_losses, train_accuracy, val_losses, val_accuracy,a_train,a_val,l_train,l_val):\n",
        "    plt.figure(figsize=(25,21))\n",
        "    plt.subplot(2,2,1)\n",
        "    plt.plot(np.arange(0, len(l_train)), l_train, label=f'train_loss')\n",
        "    plt.legend()\n",
        "    plt.subplot(2,2,3)\n",
        "    plt.plot(np.arange(0, len(l_val)), l_val, label=f'val_loss')\n",
        "    plt.legend()\n",
        "    plt.subplot(2,2,2)\n",
        "    plt.plot(np.arange(0, len(a_train)), a_train, label='train_accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(2,2,4)\n",
        "    plt.plot(np.arange(0, len(a_val)), a_val, label='val_accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    print(train_accuracy)\n",
        "    print(a)\n",
        "    "
      ],
      "metadata": {
        "id": "zdLJEqvM1jZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results(train_losses, train_accuracy, val_losses, val_accuracy,a_train,a_val,l_train,l_val)"
      ],
      "metadata": {
        "id": "myRcETU81mQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "def prf(model):\n",
        "  val_losses = []\n",
        "\n",
        "  val_accuracy = []\n",
        "  start = time.time()\n",
        "\n",
        "  a_val=[]\n",
        "\n",
        "  l_val=[]\n",
        "  with torch.no_grad():\n",
        "            correct = []\n",
        "            loss1=[]\n",
        "            for img, label in tqdm(val_loader):\n",
        "                clear_output(True)\n",
        "                img = img.to(device)\n",
        "                label = label.to(device)\n",
        "                preds = model(img).to(device)\n",
        "                print(label)\n",
        "                print(preds.cpu().detach().numpy())\n",
        "                ce_loss = nn.CrossEntropyLoss()\n",
        "                loss = ce_loss(preds, label)\n",
        "                tmp_loss = loss.item()\n",
        "                val_losses.append(tmp_loss)\n",
        "\n",
        "                acc = accuracy_score(label.cpu(), np.argmax(preds.cpu().detach().numpy(), axis=1))\n",
        "                val_accuracy.append(acc)\n",
        "                correct.append(acc)\n",
        "                loss1.append(tmp_loss)\n",
        "                plt.figure(figsize=(15,13))\n",
        "                plt.subplot(2,1,1)\n",
        "                plt.plot(np.arange(0, len(val_losses)), val_losses, label=f'ep={1}')\n",
        "                plt.legend()\n",
        "\n",
        "                plt.subplot(2,1,2)\n",
        "                plt.plot(np.arange(0, len(val_accuracy)), val_accuracy, label='val_accuracy')\n",
        "                plt.legend()\n",
        "                plt.show()\n",
        "            accuracy = sum(correct)/len(correct)\n",
        "            l=sum(loss1)/len(loss1)\n",
        "            # trainset, not train_loader\n",
        "            # probably x in your case\n",
        "            a_val.append(accuracy)\n",
        "            l_val.append(l)\n",
        "            print(\"Accuracy = {}\".format(a_val))\n",
        "\n",
        "def predict(model,image,device,encoder,transforms = None):\n",
        "    #model = torch.load('./model.h5')\n",
        "    model.eval()\n",
        "    if(isinstance(image,np.ndarray)):\n",
        "      image = Image.fromarray(image)\n",
        "    if(transforms!=None):\n",
        "        image = transforms(image)\n",
        "    data = image.expand(1,-1,-1,-1)\n",
        "    data = data.type(torch.FloatTensor).to(device)\n",
        "    sm = nn.Softmax(dim = 1)\n",
        "    output = model(data)\n",
        "    output = sm(output)\n",
        "    _, preds = torch.max(output, 1)\n",
        "    #img_plot(image)\n",
        "    result=prediction_bar(output,encoder)\n",
        "    return preds,result\n",
        "def prediction_bar(output,encoder):\n",
        "    output = output.cpu().detach().numpy()\n",
        "    a = output.argsort()\n",
        "    a = a[0]\n",
        "    \n",
        "    size = len(a)\n",
        "    if(size>5):\n",
        "        a = np.flip(a[-5:])\n",
        "    else:\n",
        "        a = np.flip(a[-1*size:])\n",
        "    prediction = list()\n",
        "    clas = list()\n",
        "    for i in a:\n",
        "      prediction.append(float(output[:,i]*100))\n",
        "      clas.append(str(i))\n",
        "    for i in a:\n",
        "        print('Class: {} , confidence: {}'.format(encoder[int(i)],float(output[:,i]*100)))\n",
        "    #plt.bar(clas,prediction)\n",
        "    #plt.title(\"Confidence score bar graph\")\n",
        "    #plt.xlabel(\"Confidence score\")\n",
        "    #plt.ylabel(\"Class number\")\n",
        "    print('_____________________________________')\n",
        "    print(encoder[int(clas[0])])\n",
        "    return encoder[int(clas[0])]\n",
        "def img_plot(image,inv_normalize = None):\n",
        "    if(inv_normalize!=None):\n",
        "        image = inv_normalize(image)\n",
        "    image = image.cpu().numpy().transpose(1,2,0)\n",
        "    plt.imshow(image)\n",
        "    plt.show()\n",
        "image = cv2.imread('/content/drive/MyDrive/les_photos_de_validation/09022_Plantae_Tracheophyta_Magnoliopsida_Myrtales_Myrtaceae_Myrtus_communis/52e9135e-4c29-4847-861c-cb1612e179f9.jpg')\n",
        "mo = torch.load('/content/drive/MyDrive/model_torch')\n",
        "pred = predict(mo,image,device,encoder,transformer)\n",
        "#prf(mo)\n",
        "true={}\n",
        "false={}\n",
        "for i in sorted(os.listdir(os.path.join(DATA_PATH + '/les_photos_de_validation'))):\n",
        "  true[i]=0\n",
        "  for a in sorted(os.listdir(os.path.join(DATA_PATH + '/les_photos_de_validation/'+i))):\n",
        "    image = cv2.imread(os.path.join(DATA_PATH + '/les_photos_de_validation/'+i+'/'+a))\n",
        "    pred,res = predict(mo,image,device,encoder,transformer)\n",
        "    print(i)\n",
        "    if(res==i):\n",
        "      true[i]=true[i]+1\n",
        "      print(\"true\")\n",
        "print(true['06115_Plantae_Tracheophyta_Liliopsida_Commelinales_Commelinaceae_Tradescantia_zebrina'])"
      ],
      "metadata": {
        "id": "gBFuIWGoYbnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s=0\n",
        "for i in sorted(os.listdir(os.path.join(DATA_PATH + '/les_photos_de_validation'))):\n",
        "  print(true[i])\n",
        "  s=s+true[i]\n",
        "print(s)"
      ],
      "metadata": {
        "id": "bYM4BN3eoOme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/drive/MyDrive/model_torch'\n",
        "torch.save(model1, PATH)\n"
      ],
      "metadata": {
        "id": "ve5vkpKWmGZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testdataset = torchvision.datasets.ImageFolder(os.path.join(DATA_PATH, 'les_photos_de_validation'), transform=transformer)"
      ],
      "metadata": {
        "id": "eIESPctnCQAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testloader = torch.utils.data.DataLoader(testdataset,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             num_workers=NUM_WORKERS)"
      ],
      "metadata": {
        "id": "6HiDrWpVCpjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "classes = os.listdir('drive/MyDrive//train') # Get names of classes\n",
        "dataiter = iter(testloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
      ],
      "metadata": {
        "id": "bkrOK3N8CDI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = model1\n",
        "net.load_state_dict(torch.load(PATH))"
      ],
      "metadata": {
        "id": "lHXX_HdEFgU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation function.\n",
        "def validate(model1, testloader, criterion):\n",
        "    model1.eval()\n",
        "    print('Validation')\n",
        "    valid_running_loss = 0.0\n",
        "    valid_running_correct = 0\n",
        "    counter = 0\n",
        "    with torch.no_grad():\n",
        "        for i, data in tqdm(enumerate(testloader), total=len(testloader)):\n",
        "            counter += 1\n",
        "            \n",
        "            image, labels = data\n",
        "            image = image.to(device)\n",
        "            labels = labels.to(device)\n",
        "            # Forward pass.\n",
        "            outputs = model(image)\n",
        "            # Calculate the loss.\n",
        "            loss = criterion(outputs, labels)\n",
        "            valid_running_loss += loss.item()\n",
        "            # Calculate the accuracy.\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            valid_running_correct += (preds == labels).sum().item()\n",
        "        \n",
        "    # Loss and accuracy for the complete epoch.\n",
        "    epoch_loss = valid_running_loss / counter\n",
        "    epoch_acc = 100. * (valid_running_correct / len(testloader.dataset))\n",
        "    return epoch_loss, epoch_acc"
      ],
      "metadata": {
        "id": "m8Oa30EAL9-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9jaSytxhYlyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net.predict(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "id": "VM6UqvoYE69P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "     transforms.Resize((SIZE_H, SIZE_W)),\n",
        "     transforms.RandomInvert(),\n",
        "     transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n",
        "     transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
        "     transforms.RandomSolarize(threshold=192.0),\n",
        "     transforms.RandomEqualize(),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize(image_mean, image_std),\n",
        "     ])\n",
        "train_dataset_aug = torchvision.datasets.ImageFolder(os.path.join(DATA_PATH, 'train'), transform=train_transform)"
      ],
      "metadata": {
        "id": "j41Ebn_P2EZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset_aug)"
      ],
      "metadata": {
        "id": "15QcVXP-2HjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader_aug = torch.utils.data.DataLoader(train_dataset_aug, \n",
        "                                              batch_size=BATCH_SIZE,\n",
        "                                              shuffle=True,\n",
        "                                              num_workers=NUM_WORKERS)"
      ],
      "metadata": {
        "id": "YvKAhYwP2auA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7C6MdC0ftCDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = EfficientNet.from_pretrained('efficientnet-b4')\n",
        "\n",
        "# Disable gradient updates for all the layers except the final layer\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_feat = model._fc.in_features\n",
        "model._fc = nn.Linear(num_feat, NUM_CLASSES)\n",
        "\n",
        "# Use available device for calculations\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "QV0mQXxs7fE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCH_NUM=20\n",
        "augmentation=1\n",
        "shedul=0\n",
        "model2, train_losses2, train_accuracy2, val_losses2, val_accuracy2 = fit(model, augmentation, shedul, EPOCH_NUM , lr=0.0003)"
      ],
      "metadata": {
        "id": "PT8lYydd7iIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_results(train_losses2, train_accuracy2, val_losses2, val_accuracy2)"
      ],
      "metadata": {
        "id": "_ZIr0mjDBp06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a function to save a model\n",
        "import datetime\n",
        "def save_model(model):\n",
        "  \"\"\"\n",
        "  saves a given model in a models directory and appends a suffix (string).\n",
        "  \"\"\"\n",
        "  # Create a model directory pathname with current time\n",
        "  modeldir = os.path.join(\"/content/drive/MyDrive/model\",\n",
        "                          datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\"))\n",
        "  model_path = modeldir + \"-\" +  \".h5\" # save format of model\n",
        "  print(f\"saving model to : {model_path}...\")\n",
        "  model.save(model_path)\n",
        "  return model_path\n"
      ],
      "metadata": {
        "id": "8snN3RnjbnhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model1, '/content/drive/MyDrive/model.h5')"
      ],
      "metadata": {
        "id": "KL2zpo-GceI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def edge_and_cut(img):\n",
        "    emb_img = img.copy()\n",
        "    edges = cv2.Canny(img, 200, 300)\n",
        "    \n",
        "    return edges\n",
        "filepath = \"drive/MyDrive/folder_image_Morocco_training_image_plants/05791_Plantae_Tracheophyta_Liliopsida_Alismatales_Araceae_Alocasia_odora/0cc30b70-03dc-409a-9488-cdee3e6124d5.jpg\"\n",
        "image = cv2.imread(filepath) \n",
        "img=edge_and_cut(image)\n",
        "print(img.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "x41YowoKAfXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sEHy2llpRQHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain=[]\n",
        "Xtest=[]\n",
        "ytrain=[]\n",
        "ytest=[]\n",
        "Xtest=[]\n",
        "Xvalid=[]\n",
        "ytest=[]\n",
        "yvalid=[]\n",
        "\n",
        "def label_img(classes_name,category):\n",
        "    arr = np.zeros((len(classes_name),), dtype = int)\n",
        "    arr[classes.index(category), ] = 1\n",
        "    return arr\n",
        "classes_num = 85\n",
        "def create_data(classes_name,l,folder,X,y):\n",
        "    for category in classes_name:\n",
        "        imgs = 0\n",
        "        category_path = os.path.join(folder, category)\n",
        "        label = label_img(classes_name,category)\n",
        "        for img in os.listdir(category_path):\n",
        "            imgs = imgs + 1\n",
        "            if imgs == l:\n",
        "                break\n",
        "            frame = cv2.imread(os.path.join(category_path, img))\n",
        "            #print(frame.shape)\n",
        "            img = cv2.resize(frame, ( 150,150))\n",
        "            #img_array=edge_and_cut(img)\n",
        "            img_array=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            print(img_array.shape)\n",
        "            X.append(img_array/255.0)\n",
        "            y.append(label)\n",
        "data_dir =  r'drive/MyDrive/train'\n",
        "\n",
        "\n",
        "classes = os.listdir(data_dir) # Get names of classes\n",
        "len(classes)\n",
        "create_data(classes,100,data_dir,Xtrain,ytrain)"
      ],
      "metadata": {
        "id": "wS2-JNFe8kD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-ZllwpmmAGy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir =  r'drive/MyDrive/test'\n",
        "\n",
        "\n",
        "classes = os.listdir(data_dir) # Get names of classes\n",
        "len(classes)\n",
        "create_data(classes,30,data_dir,Xtest,ytest)"
      ],
      "metadata": {
        "id": "dc5lSkUzcSIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir =  r'drive/MyDrive/val'\n",
        "\n",
        "\n",
        "classes = os.listdir(data_dir) # Get names of classes\n",
        "len(classes)\n",
        "create_data(classes,30,data_dir,Xvalid,yvalid)"
      ],
      "metadata": {
        "id": "skT3V08acbGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 \n",
        "import matplotlib.pyplot as plt\n",
        "filepath = \"drive/MyDrive/folder_image_Morocco_training_image_plants/05791_Plantae_Tracheophyta_Liliopsida_Alismatales_Araceae_Alocasia_odora/0cc30b70-03dc-409a-9488-cdee3e6124d5.jpg\"\n",
        "image = cv2.imread(filepath) \n",
        "print(type(image))\n",
        "height, width = image.shape[:2] \n",
        "  \n",
        "print(\"The height of the image is: \", height) \n",
        "print(\"The width of the image is: \", width) \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EEOypD-UTHYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain=np.asarray(Xtrain)\n",
        "ytrain=np.asarray(ytrain)\n",
        "Xtest=np.asarray(Xtest)\n",
        "ytest=np.asarray(ytest)\n",
        "Xvalid=np.asarray(Xvalid)\n",
        "yvalid=np.asarray(yvalid)\n",
        "print(Xtrain.shape)\n",
        "print(ytrain.shape)\n",
        "print(Xtest.shape)\n",
        "print(ytest.shape)\n",
        "print(Xvalid.shape)\n",
        "print(yvalid.shape)\n",
        "#train = np.array(train)\n",
        "#shuffle(train)\n",
        "print(Xtrain.shape[1:])\n"
      ],
      "metadata": {
        "id": "iPUKRgT15_yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow.keras.layers as L\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "TrFh3D0heKv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TL pecific modules\n",
        "from keras.applications.vgg16 import VGG16\n",
        "import time\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.optimizers as Optimizer\n",
        "weights_path='/content/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "base_model=VGG16(include_top=False, weights=None,input_shape=(150,150,3), pooling='avg')\n",
        "base_model.load_weights(weights_path)\n",
        "base_model.summary()\n",
        "model=Sequential()\n",
        "model.add(base_model)\n",
        "\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(Dense(85,activation='softmax'))\n",
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.1, # Randomly zoom image \n",
        "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "\n",
        "datagen.fit(Xtrain)\n",
        "epochs=150\n",
        "batch_size=32\n",
        "red_lr=ReduceLROnPlateau(monitor='val_acc', factor=0.1, epsilon=0.0001, patience=2, verbose=1)\n",
        "model.summary()\n",
        "base_model.trainable=False # setting the VGG model to be untrainable.\n",
        "model.compile(optimizer=Adam(lr=1e-4),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "History = model.fit_generator(datagen.flow(Xtrain,ytrain, batch_size=batch_size),\n",
        "                              epochs = 150, validation_data = (Xtest,ytest),\n",
        "                              verbose = 1, steps_per_epoch=Xtrain.shape[0] // batch_size)"
      ],
      "metadata": {
        "id": "KRUwjmShMAdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=(350, 350,3),\n",
        "                                             weights='imagenet',\n",
        "                                             include_top=False),\n",
        "                                             L.GlobalAveragePooling2D(),\n",
        "                                             L.Dense(85,\n",
        "                                             activation='softmax')])\n",
        "        \n",
        "model.compile(optimizer='adam',\n",
        "                  loss = 'categorical_crossentropy',\n",
        "                  metrics=['categorical_accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "WU5jvJkreNgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.optimizers as Optimizer\n",
        "model.compile( optimizer=Optimizer.Adam(lr=0.0001),loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Tt5EWj-JeRsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# Time to train our model !\n",
        "epochs = 100#100\n",
        "batch_size=64#32\n",
        " \n",
        "train_datagen = ImageDataGenerator(\n",
        "        rotation_range=10,  \n",
        "        zoom_range = 0.1, \n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,  \n",
        "        horizontal_flip=True\n",
        ")\n",
        " \n",
        "test_datagen = ImageDataGenerator()\n",
        " \n",
        "train_generator = train_datagen.flow(\n",
        "    Xtrain,ytrain,\n",
        "    batch_size=batch_size)\n",
        " \n",
        "validation_generator = test_datagen.flow(\n",
        "    Xvalid,yvalid,\n",
        "    batch_size=batch_size)\n",
        " \n",
        "checkpointer = ModelCheckpoint(filepath = \"/dataset_anass/PId_Best.h5\", save_best_only = True, verbose = 1)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy', patience = 3, verbose = 1, factor = 0.5, minlr = 0.00001)\n",
        " \n",
        "\n",
        "start = time.time()\n",
        " \n",
        "# let's get started !\n",
        " \n",
        "history=model.fit_generator(train_generator,\n",
        "                            epochs=epochs,\n",
        "                            validation_data = validation_generator,\n",
        "                            verbose=1,\n",
        "                            steps_per_epoch=len(Xtrain) // batch_size,\n",
        "                            validation_steps=len(Xvalid) //batch_size,\n",
        "                            callbacks=[checkpointer, learning_rate_reduction]\n",
        "                           )\n",
        " \n",
        "end = time.time()\n",
        " \n",
        "duration = end - start\n",
        "print ('\\n This Model took %0.2f seconds (%0.1f minutes) to train for %d epochs'%(duration, duration/60, epochs) )"
      ],
      "metadata": {
        "id": "J2Zd-Eq3eZO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n"
      ],
      "metadata": {
        "id": "4hhoI5QJ5_79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', input_shape=(150, 150,3), activation='relu', name='Conv2D_1'))\n",
        "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', name='Conv2D_2'))\n",
        "model.add(MaxPool2D(pool_size=(2,2), name='Maxpool_1'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu', name='Conv2D_3'))\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', name='Conv2D_4'))\n",
        "model.add(MaxPool2D(pool_size=(2,2), name='Maxpool_2'))\n",
        "model.add(Dropout(0.25))\n",
        "    \n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu', name='Conv2D_5'))\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu', name='Conv2D_6'))\n",
        "model.add(MaxPool2D(pool_size=(2,2), name='Maxpool_3'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=512, activation='relu', name='Dense_1'))\n",
        "model.add(Dropout(0.5))\n",
        "#model.add(Dense(units=128, activation='relu', name='Dense_2'))\n",
        "model.add(Dense(units=85, activation='softmax', name='Output'))"
      ],
      "metadata": {
        "id": "DWTGJ8F78DWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "XdsL7N51BRr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.optimizers as Optimizer\n",
        "model.compile( optimizer=Optimizer.Adam(lr=0.0001),loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "dQQgu2WFBb0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        " \n",
        "# Time to train our model !\n",
        "epochs = 100#100\n",
        "batch_size=64#32\n",
        " \n",
        "train_datagen = ImageDataGenerator(\n",
        "        rotation_range=10,  \n",
        "        zoom_range = 0.1, \n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,  \n",
        "        horizontal_flip=True\n",
        ")\n",
        " \n",
        "test_datagen = ImageDataGenerator()\n",
        " \n",
        "train_generator = train_datagen.flow(\n",
        "    Xtrain,ytrain,\n",
        "    batch_size=batch_size)\n",
        " \n",
        "validation_generator = test_datagen.flow(\n",
        "    Xvalid,yvalid,\n",
        "    batch_size=batch_size)\n",
        " \n",
        "checkpointer = ModelCheckpoint(filepath = \"/dataset_anass/PId_Best.h5\", save_best_only = True, verbose = 1)\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy', patience = 3, verbose = 1, factor = 0.5, minlr = 0.00001)\n",
        " \n",
        "\n",
        "start = time.time()\n",
        " \n",
        "# let's get started !\n",
        " \n",
        "history=model.fit_generator(train_generator,\n",
        "                            epochs=epochs,\n",
        "                            validation_data = validation_generator,\n",
        "                            verbose=1,\n",
        "                            steps_per_epoch=len(Xtrain) // batch_size,\n",
        "                            validation_steps=len(Xvalid) //batch_size,\n",
        "                            callbacks=[checkpointer, learning_rate_reduction]\n",
        "                           )\n",
        " \n",
        "end = time.time()\n",
        " \n",
        "duration = end - start\n",
        "print ('\\n This Model took %0.2f seconds (%0.1f minutes) to train for %d epochs'%(duration, duration/60, epochs) )"
      ],
      "metadata": {
        "id": "55jctojEwZBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MTLJ8wGzBfKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SOZdVVm2Eg-H"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "plants",
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}